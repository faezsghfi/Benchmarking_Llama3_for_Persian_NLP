{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/kaggle/input/sa-data/food.jsonl\"\n",
    "\n",
    "with open(input_file) as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_inter = pd.DataFrame(lines)\n",
    "df_inter.columns = ['json_element']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df_inter['json_element'].apply(json.loads)\n",
    "\n",
    "df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "df_final.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[['review', 'sentiment']]\n",
    "df_final.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"transformers\" --upgrade\n",
    "!pip install accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\"\n",
    "# model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "# model_id = \"unsloth/gemma-2-9b-bnb-4bit\"\n",
    "# model_id = \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# model_path = \"PartAI/Dorna-Llama3-8B-Instruct\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"MaralGPT/Maral-7B-alpha-1\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"MehdiHosseiniMoghadam/AVA-Llama-3-V2\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"CohereForAI/aya-23-8b\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"universitytehran/PersianMind-v1.0\"\n",
    "# token = \"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\"\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16,device_map=\"auto\", use_auth_token=token)\n",
    "# pipeline = transformers.pipeline(\"text-generation\", model=model,model_kwargs={\"torch_dtype\": torch.float16,\"quantization_config\": {\"load_in_4bit\": True},\"low_cpu_mem_usage\": True,}, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_path = \"PartAI/Dorna-Llama3-8B-Instruct\"\n",
    "token = \"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\"\n",
    "\n",
    "# model_path = \"MaralGPT/Maral-7B-alpha-1\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"MehdiHosseiniMoghadam/AVA-Llama-3-V2\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"CohereForAI/aya-23-8b\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"universitytehran/PersianMind-v1.0\"\n",
    "# token = \"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\"\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16,device_map=\"auto\", use_auth_token=token)\n",
    "# pipeline = transformers.pipeline(\"text-generation\", model=model,model_kwargs={\"torch_dtype\": torch.float16,\"quantization_config\": {\"load_in_4bit\": True},\"low_cpu_mem_usage\": True,}, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(prompt):\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \".تو یک دستیار ویرایشگر متن های فارسی هستی\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"{prompt}\"\"\".format(prompt = prompt)},\n",
    "  ]\n",
    "\n",
    "  prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "  generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        top_k=1,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=300,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  return(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_FEW = \"\"\"\n",
    "جمله زیر نظر یک شخص است. این جمله به زبان فارسی است. بار یا احساس موجود در این جمله را شناسایی کن. \\\n",
    "پاسخ‌ های ممکن کلمات روبرو هستند: POSITIVE, NEUTRAL, NEGATIVE, OTHER. \\\n",
    "زمانی که جمله دارای احساس خاصی نیست یا شامل احساسات مختلف است از OTHER استفاده کن \\\n",
    "به مثالهای زیر توجه کن:\n",
    "\n",
    "  مثال 1:\n",
    "      Review: پودر قهوه دارای کیفیت مطلوب نبود وعطر و بوی در فضا پخش نمی شد\n",
    "      Label: NEGATIVE\n",
    "\n",
    "\n",
    "  مثال 2:\n",
    "      Review: از این برند انتظار بیشتری داشتم خیلی خشک و بی مزه بود\n",
    "      Label: NEGATIVE\n",
    "\n",
    "\n",
    "  مثال 3:\n",
    "      Review: تعداد شیر های پرچرب کمتر از کم چرب بود ۴ تا کم چرب بود و ۲ تا پر چرب\n",
    "      Label: NEUTRAL\n",
    "\n",
    "\n",
    "  مثال 4:\n",
    "      Review: دفعه های قبل که سفارش دادم بو میداد ولی ایندفعه تازه بود و بوی بد هم نمیداد\n",
    "      Label: POSITIVE\n",
    "\n",
    "\n",
    "  مثال 5:\n",
    "      Review: به نظرم خیلی خوشمزه بود طعم شکلات با قهوه عالی میشه\n",
    "      Label: POSITIVE\n",
    "\n",
    "\n",
    "  مثال 6:\n",
    "      Review: از نظر کیفیت خوبه ولی از نظر قیمت فقط تو پیشنهاد ویژه خوبه\n",
    "      Label: OTHER\n",
    "\n",
    "\n",
    "  مثال 7:\n",
    "      Review: نسبت به قیمت جدیدش قهوه های بهتری رو توی این رنج قیمت میشه خرید\n",
    "      Label: NEGATIVE\n",
    "\n",
    "\n",
    "  فرمت خروجی:\n",
    "  فرمت خروجی شما باید یک تاپل  باشد، که در آن هر تاپل از متن نظر ورودی و برچسب احساسات لیست شده ای که تشخیص داده ای تشکیل شده باشد.\n",
    "\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_response = []\n",
    "\n",
    "for index, row in df_final.iterrows():\n",
    "    print('i: ',index)\n",
    "    label = row['sentiment']\n",
    "    review = row['review']\n",
    "\n",
    "    prompt_arman = f\"\"\"\n",
    "    شرح وظیفه:\n",
    "    {PROMPT_FEW}\n",
    "\n",
    "    ورودی:\n",
    "    {review}\n",
    "    \"\"\"\n",
    "    response = get_inference(prompt_arman)\n",
    "\n",
    "    all_response.append({'review':review , 'label':label , 'response':response})\n",
    "    np.save(f'/kaggle/working/sa_resault.npy', np.array(all_response, dtype=object))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
