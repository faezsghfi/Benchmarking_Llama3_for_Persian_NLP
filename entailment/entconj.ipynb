{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-06T17:56:12.488879Z",
     "iopub.status.busy": "2024-12-06T17:56:12.488545Z",
     "iopub.status.idle": "2024-12-06T17:57:35.459328Z",
     "shell.execute_reply": "2024-12-06T17:57:35.458492Z",
     "shell.execute_reply.started": "2024-12-06T17:56:12.488849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (0.43.0)\n",
      "Collecting hazm\n",
      "  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
      "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting flashtext<3.0,>=2.7 (from hazm)\n",
      "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from hazm) (4.3.3)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from hazm)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy==1.24.3 (from hazm)\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
      "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from hazm) (1.2.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (70.0.0)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n",
      "Downloading hazm-0.10.0-py3-none-any.whl (892 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flashtext\n",
      "  Building wheel for flashtext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=e20ba4e5936a3054d5d30297d8d55c3e158081f9b573f9880ebcc17861ad5374\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
      "Successfully built flashtext\n",
      "Installing collected packages: flashtext, python-crfsuite, numpy, nltk, scipy, fasttext-wheel, hazm\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.10.1 requires cubinlinker, which is not installed.\n",
      "cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 24.10.1 requires libcudf==24.10.*, which is not installed.\n",
      "cudf 24.10.1 requires ptxcompiler, which is not installed.\n",
      "cuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cuml 24.10.0 requires cuvs==24.10.*, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cublas, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cufft, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-curand, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cusolver, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cusparse, which is not installed.\n",
      "dask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "pylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\n",
      "pylibraft 24.10.0 requires nvidia-cublas, which is not installed.\n",
      "pylibraft 24.10.0 requires nvidia-curand, which is not installed.\n",
      "pylibraft 24.10.0 requires nvidia-cusolver, which is not installed.\n",
      "pylibraft 24.10.0 requires nvidia-cusparse, which is not installed.\n",
      "ucxx 0.40.0 requires libucxx==0.40.*, which is not installed.\n",
      "albucore 0.0.20 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "albumentations 1.4.21 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\n",
      "bayesian-optimization 2.0.0 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\n",
      "cudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\n",
      "cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "dask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "featuretools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\n",
      "pylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\n",
      "rmm 24.10.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "woodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "xarray 2024.11.0 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 hazm-0.10.0 nltk-3.9.1 numpy-1.24.3 python-crfsuite-0.9.11 scipy-1.13.1\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting googletrans\n",
      "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httpx==0.13.3 (from googletrans)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2024.6.2)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
      "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=b931549504461a0f0a59f9e7c517d52fd976270ed7ab099125ada6e30c132928\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.111.0 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "jupyterlab 4.3.1 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\n",
      "jupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install wheel\n",
    "! pip install hazm\n",
    "! pip install transformers\n",
    "! pip install sentencepiece\n",
    "! pip install numpy>=1.17\n",
    "!pip install datasets\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T17:59:10.735918Z",
     "iopub.status.busy": "2024-12-06T17:59:10.735063Z",
     "iopub.status.idle": "2024-12-06T17:59:39.353915Z",
     "shell.execute_reply": "2024-12-06T17:59:39.353271Z",
     "shell.execute_reply.started": "2024-12-06T17:59:10.735881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,  MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:06:06.383103Z",
     "iopub.status.busy": "2024-12-06T19:06:06.382416Z",
     "iopub.status.idle": "2024-12-06T19:06:06.405496Z",
     "shell.execute_reply": "2024-12-06T19:06:06.404578Z",
     "shell.execute_reply.started": "2024-12-06T19:06:06.383059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_file = \"/kaggle/input/conj-data/data.jsonl\"\n",
    "\n",
    "with open(input_file) as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:06:10.548695Z",
     "iopub.status.busy": "2024-12-06T19:06:10.548373Z",
     "iopub.status.idle": "2024-12-06T19:06:10.561087Z",
     "shell.execute_reply": "2024-12-06T19:06:10.560160Z",
     "shell.execute_reply.started": "2024-12-06T19:06:10.548668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_inter = pd.DataFrame(lines)\n",
    "df_inter.columns = ['json_element']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:06:13.442972Z",
     "iopub.status.busy": "2024-12-06T19:06:13.441972Z",
     "iopub.status.idle": "2024-12-06T19:06:13.480481Z",
     "shell.execute_reply": "2024-12-06T19:06:13.479673Z",
     "shell.execute_reply.started": "2024-12-06T19:06:13.442922Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>En_premise</th>\n",
       "      <th>En_hypothesis</th>\n",
       "      <th>Fa_premise</th>\n",
       "      <th>Fa_hypothesis</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Historically, the Commission was run by three...</td>\n",
       "      <td>Historically, the Commission was run by three...</td>\n",
       "      <td>از لحاظ تاریخی، کمیسیون توسط سه کمیسیونر یا ک...</td>\n",
       "      <td>از نظر تاریخی، کمیسیون توسط سه کمیشنر اداره م...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Quebec, an allophone is a resident, usuall...</td>\n",
       "      <td>In Quebec, an allophone is a resident, usuall...</td>\n",
       "      <td>در کبک، آلوفون ساکنی است، معمولاً یک مهاجر، ک...</td>\n",
       "      <td>در کبک، آلوفون ساکنی است که معمولاً یک مهاجر ...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The program usually aired at 10pm the night o...</td>\n",
       "      <td>The program usually aired at 10pm the night o...</td>\n",
       "      <td>برنامه معمولاً ساعت 22 شب بازی لیگ قهرمانان ا...</td>\n",
       "      <td>برنامه معمولاً ساعت 22 شب بازی لیگ قهرمانان ا...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terry Phelps and Raffaella Reggi were the def...</td>\n",
       "      <td>Terry Phelps and Raffaella Reggi were the def...</td>\n",
       "      <td>تری فلپس و رافائلا رگی مدافع عنوان قهرمانی بو...</td>\n",
       "      <td>تری فلپس و رافائلا رگی مدافع عنوان قهرمانی بو...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Only magnitude 6.0 earthquakes appear on the ...</td>\n",
       "      <td>Only magnitude 6.0 or greater earthquakes app...</td>\n",
       "      <td>فقط زلزله‌های 6 ریشتری در لیست ظاهر می‌شوند.</td>\n",
       "      <td>فقط زمین‌لرزه‌هایی با بزرگی 6 ریشتر یا بیشتر ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is suitable for cultivation in the alpinum...</td>\n",
       "      <td>It is suitable for cultivation in the alpinum...</td>\n",
       "      <td>برای کشت در باغ آلپینیوم مناسب است.</td>\n",
       "      <td>برای کشت در باغ آلپینیوم یا باغ صخره ای مناسب...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The John Deere Gator has been made in a varie...</td>\n",
       "      <td>The John Deere Gator has been made in a varie...</td>\n",
       "      <td>John Deere Gator در پیکربندی های مختلف با چها...</td>\n",
       "      <td>John Deere Gator در پیکربندی های مختلف با چها...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A variant type is prepared with concentrated ...</td>\n",
       "      <td>A variant type is prepared with concentrated ...</td>\n",
       "      <td>یک نوع با قهوه مایع غلیظ تهیه می شود.</td>\n",
       "      <td>یک نوع با قهوه مایع غلیظ یا پودر قهوه فوری ته...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It is the eighth installment in \"The Fast\" fr...</td>\n",
       "      <td>It is the eighth installment in \"The Fast and...</td>\n",
       "      <td>هشتمین قسمت از فرنچایز \"سریع\" است.</td>\n",
       "      <td>این قسمت هشتم از فرنچایز \"سریع و خشمگین\" است.</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In 2008 she ran for parliament, but was not e...</td>\n",
       "      <td>In 2008 she ran for parliament, but was elected.</td>\n",
       "      <td>در سال 1387 نامزد مجلس شد اما انتخاب نشد.</td>\n",
       "      <td>در سال 1387 نامزد مجلس شد اما انتخاب شد.</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          En_premise  \\\n",
       "0   Historically, the Commission was run by three...   \n",
       "1   In Quebec, an allophone is a resident, usuall...   \n",
       "2   The program usually aired at 10pm the night o...   \n",
       "3   Terry Phelps and Raffaella Reggi were the def...   \n",
       "4   Only magnitude 6.0 earthquakes appear on the ...   \n",
       "5   It is suitable for cultivation in the alpinum...   \n",
       "6   The John Deere Gator has been made in a varie...   \n",
       "7   A variant type is prepared with concentrated ...   \n",
       "8   It is the eighth installment in \"The Fast\" fr...   \n",
       "9   In 2008 she ran for parliament, but was not e...   \n",
       "\n",
       "                                       En_hypothesis  \\\n",
       "0   Historically, the Commission was run by three...   \n",
       "1   In Quebec, an allophone is a resident, usuall...   \n",
       "2   The program usually aired at 10pm the night o...   \n",
       "3   Terry Phelps and Raffaella Reggi were the def...   \n",
       "4   Only magnitude 6.0 or greater earthquakes app...   \n",
       "5   It is suitable for cultivation in the alpinum...   \n",
       "6   The John Deere Gator has been made in a varie...   \n",
       "7   A variant type is prepared with concentrated ...   \n",
       "8   It is the eighth installment in \"The Fast and...   \n",
       "9   In 2008 she ran for parliament, but was elected.   \n",
       "\n",
       "                                          Fa_premise  \\\n",
       "0   از لحاظ تاریخی، کمیسیون توسط سه کمیسیونر یا ک...   \n",
       "1   در کبک، آلوفون ساکنی است، معمولاً یک مهاجر، ک...   \n",
       "2   برنامه معمولاً ساعت 22 شب بازی لیگ قهرمانان ا...   \n",
       "3   تری فلپس و رافائلا رگی مدافع عنوان قهرمانی بو...   \n",
       "4       فقط زلزله‌های 6 ریشتری در لیست ظاهر می‌شوند.   \n",
       "5                برای کشت در باغ آلپینیوم مناسب است.   \n",
       "6   John Deere Gator در پیکربندی های مختلف با چها...   \n",
       "7              یک نوع با قهوه مایع غلیظ تهیه می شود.   \n",
       "8                 هشتمین قسمت از فرنچایز \"سریع\" است.   \n",
       "9          در سال 1387 نامزد مجلس شد اما انتخاب نشد.   \n",
       "\n",
       "                                       Fa_hypothesis target  \n",
       "0   از نظر تاریخی، کمیسیون توسط سه کمیشنر اداره م...      n  \n",
       "1   در کبک، آلوفون ساکنی است که معمولاً یک مهاجر ...      e  \n",
       "2   برنامه معمولاً ساعت 22 شب بازی لیگ قهرمانان ا...      e  \n",
       "3   تری فلپس و رافائلا رگی مدافع عنوان قهرمانی بو...      n  \n",
       "4   فقط زمین‌لرزه‌هایی با بزرگی 6 ریشتر یا بیشتر ...      c  \n",
       "5   برای کشت در باغ آلپینیوم یا باغ صخره ای مناسب...      n  \n",
       "6   John Deere Gator در پیکربندی های مختلف با چها...      e  \n",
       "7   یک نوع با قهوه مایع غلیظ یا پودر قهوه فوری ته...      e  \n",
       "8      این قسمت هشتم از فرنچایز \"سریع و خشمگین\" است.      c  \n",
       "9           در سال 1387 نامزد مجلس شد اما انتخاب شد.      c  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "df_inter['json_element'].apply(json.loads)\n",
    "\n",
    "df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "df_final.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:06:19.855871Z",
     "iopub.status.busy": "2024-12-06T19:06:19.855043Z",
     "iopub.status.idle": "2024-12-06T19:12:39.170592Z",
     "shell.execute_reply": "2024-12-06T19:12:39.169560Z",
     "shell.execute_reply.started": "2024-12-06T19:06:19.855835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee85b8c48af47bb8db4f2f55a29cdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4e2446b4dc4a38a237b58042c07fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56de6d658b714af6a5dc87071d13cbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd583fe48ac429b886e4724dc9f75b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fdd0ce33a24ea0861d3e0e908381c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8894eaa612394299b3fe6a7f2e8b2f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5a0fdc6a9043378a82041f165df509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd07ef27e154198ae72378a9775cb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f1a8df5cc24a419235c25e589d3808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d72b11f9484b7eba2a7bd1ed46dd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a69a5b740174fd687ccc914202f455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148a7ad729be43aea5811462eb85ffaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c709e298d79475a8280e86d085c1ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e094fd6f1fea48d989299711bc94d6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9ea6395e0049f28b648bc8b28bba32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31c2227c7fd45738ab0036fa56dad9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18da3115a7349a5b46e75268f972cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\"\n",
    "# model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "# model_id = \"unsloth/gemma-2-9b-bnb-4bit\"\n",
    "# model_id = \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "# model_path = \"PartAI/Dorna-Llama3-8B-Instruct\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"MaralGPT/Maral-7B-alpha-1\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"MehdiHosseiniMoghadam/AVA-Llama-3-V2\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"CohereForAI/aya-23-8b\"\n",
    "# token = \"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\"\n",
    "\n",
    "# model_path = \"universitytehran/PersianMind-v1.0\"\n",
    "# token = \"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\"\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16,device_map=\"auto\", use_auth_token=token)\n",
    "# pipeline = transformers.pipeline(\"text-generation\", model=model,model_kwargs={\"torch_dtype\": torch.float16,\"quantization_config\": {\"load_in_4bit\": True},\"low_cpu_mem_usage\": True,}, tokenizer=tokenizer)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:05:42.178606Z",
     "iopub.status.busy": "2024-12-06T19:05:42.177847Z",
     "iopub.status.idle": "2024-12-06T19:05:42.468254Z",
     "shell.execute_reply": "2024-12-06T19:05:42.467355Z",
     "shell.execute_reply.started": "2024-12-06T19:05:42.178569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_inference(prompt):\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \".تو یک دستیار ویرایشگر متن های فارسی هستی\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"{prompt}\"\"\".format(prompt = prompt)},\n",
    "  ]\n",
    "\n",
    "  prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "  generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        top_k=1,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=2300,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  return(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:05:45.967595Z",
     "iopub.status.busy": "2024-12-06T19:05:45.967262Z",
     "iopub.status.idle": "2024-12-06T19:05:45.974305Z",
     "shell.execute_reply": "2024-12-06T19:05:45.973445Z",
     "shell.execute_reply.started": "2024-12-06T19:05:45.967568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_FEW = \"\"\"\n",
    "هدف وظیفه‌ی استنتاج زبان طبیعی تشخیص رابطه‌ی نتیجه‌گیری بین یک <فرضیه> با توجه به یک <پیش‌فرض> است.\n",
    "رابطه‌ یا برچسب میان آن‌ها می‌تواند یکی از سه نوع تناظر، تناقض یا ناشناخته باشد.\n",
    "- تناظر : اگر جمله <فرضیه> به طور منطقی نتیجه‌ای از جمله <پیش‌فرض> باشد\n",
    "- تناقض : اگر جمله <فرضیه> با جمله <پیش‌فرض> در تناقض باشد\n",
    "- ناشناخته : اگر رابطه‌ای قطعی بین جمله <پیش‌فرض> و جمله <فرضیه> وجود نداشته باشد و هیچ تناظر یا تناقضی نتوان برقرار کرد\n",
    "\n",
    "مثال:\n",
    "<پیش‌فرض><فرضیه>\n",
    "<برچسب>:\n",
    "    تناقض یا تناظر یا ناشناخته\n",
    "\n",
    "<گرچه فیلم موفق بود اما نقش او در این میان نادیده گرفته شد.><در سال ۱۹۴۹ در فیلم اسکاری نامه‌ای به سه همسر ظاهر شد.>\n",
    "<برچسب>:\n",
    "    ناشناخته\n",
    "\n",
    "<من همیشه شنیده ام که شما انقلابیون زندگی را ارزان پنداشته اید ، اما به نظر می رسد وقتی پای زندگی خود شما در میان باشد، قضیه فرق میکند><من دائماً شنیده ام که شما انقلابیون برای زندگی بسیار ارزش قائل هستید.>\n",
    "<برچسب>:\n",
    "    تناقض\n",
    "\n",
    "<در پایان او آهی طولانی سر داد.><او در پایان آهسته آهی کشید.>\n",
    "<برچسب>:\n",
    "    تناظر\n",
    "\n",
    "<در پس این حمله، کیافخرالدین جلال و سپس کیاوشتاسف به همراه فرزندانشان کشته‌شدند.><آنها ازین مبارزه جان سالم بدر می برند.>\n",
    "<برچسب>:\n",
    "    تناقض\n",
    "\n",
    "<باغ وحش ادینبورگ هر روز در طول تابستان رژه پنگوئن ها برگزار می کند.><همه بازدید کنندگان تابستانی باغ وحش ادینبورگ، رژه پنگوئن ها را می بینند.>\n",
    "<برچسب>:\n",
    "    ناشناخته\n",
    "\n",
    "<اخیرا برخی از نویسندگان اطلاعات مختصری از این قلعه ارائه داده‌اند که بیشتر منحصر به شکل ظاهری آن است و بنابراین از حیث علمی قابل استناد نیست.><اطلاعات ارائه شده بیشتر مربوط به قدمت ساخت قلعه بود.>\n",
    "<برچسب>:\n",
    "    تناقض\n",
    "\n",
    "<این مزایا نتیجه کنگره یا ادارات و آژانس های فدرال است که توصیه های ما را برای کارآمدتر کردن خدمات دولت ، بهبود بودجه و هزینه های دلار مالیاتی و تقویت مدیریت منابع فدرال انجام می دهند.><ما مسئول پیشرفت در مدیریت منابع فدرال هستیم.>\n",
    "<برچسب>:\n",
    "    تناظر\n",
    "\n",
    "<دوستم اینگونه راضیم کرد که من از ارتفاعات فقط وقتی می‌ترسم که به آن فکر می‌کنم ولی وقتی آن بالا می‌رسم برطرف می‌شود.><من از ارتفاعات می‌ترسیدم ولی دوستم مرا قانع کرد.>\n",
    "<برچسب>:\n",
    "    تناظر\n",
    "\n",
    "<\"یوتی‌ایر اوییشن\"، یک شرکت هواپیمایی روسی است که در سال ۱۹۶۷ توسط خطوط هواپیمایی آئروفلوت تأسیس شد و در حال حاضر روزانه به ۷۲ مقصد، در آسیای مرکزی، آسیای جنوبی، آسیای جنوب شرقی، غرب آسیا و اروپا پروازهای مستقیم دارد.><این شرکت یک شرکت دولتی است که بیش از پنجهزار نفر کارمند دارد. >\n",
    "<برچسب>:\n",
    "    ناشناخته\n",
    "\n",
    "\n",
    "  فرمت خروجی:\n",
    "  فرمت خروجی شما باید یک تاپل  باشد، که در آن هر تاپل از فرضیه و پیش فرض و پاسخ تشخیص داده شده تشکیل شده باشد.\n",
    "\n",
    "\n",
    "برای نمونه تست زیر نام محتمل‌ترین برچسب را چاپ کن:\n",
    "\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T19:19:44.520859Z",
     "iopub.status.busy": "2024-12-06T19:19:44.520500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_response = []\n",
    "\n",
    "for index, row in df_final.iterrows():\n",
    "    print('i: ',index)\n",
    "    sent1 = row['Fa_hypothesis']\n",
    "    sent2 = row['Fa_premise']\n",
    "    label = row['target']\n",
    "\n",
    "    prompt_arman = f\"\"\"\n",
    "    شرح وظیفه:\n",
    "    {PROMPT_FEW}\n",
    "\n",
    "     جمله اول:\n",
    "    {sent1}\n",
    "\n",
    "    \n",
    "\n",
    "    جمله دوم:\n",
    "    {sent2}\n",
    "\n",
    "    پاسخ:\n",
    "    \"\"\"\n",
    "    response = get_inference(prompt_arman)\n",
    "\n",
    "    all_response.append({'sent1':sent1 ,'sent2':sent2, 'label':label , 'response':response})\n",
    "    if index%10==0 :\n",
    "        np.save(f'ent_conj_maral.npy', np.array(all_response, dtype=object))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6235124,
     "sourceId": 10107586,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
